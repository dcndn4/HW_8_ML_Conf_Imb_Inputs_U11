# -*- coding: utf-8 -*-
"""
Created on Sat Nov 27 12:40:53 2021

@author: CS_Knit_tinK_SC
"""

# Credit Risk Resampling Techniques

#%%

import warnings
warnings.filterwarnings('ignore')

#%%

import numpy as np
import pandas as pd
from pathlib import Path
from collections import Counter
import matplotlib.pyplot as plt

#%%
#%%

# Read the CSV into DataFrame

#%%
#%%

# Load the data
file_path="C:/Users/CS_Knit_tinK_SC/Documents/GitHub/HW_8_ML_Conf_Imb_Inputs_U11/Resources/lending_data.csv"
#file_path = Path('Resources/lending_data.csv')
df = pd.read_csv(file_path)
print(df.head())

#%%
#%%

# Split the Data into Training and Testing¶

#%%
#%%

# Create our features
X = df.drop(columns="loan_status")

# Create our target
y = df["loan_status"]

#%%

print(X.describe())

#%%

print(y.head())

#%%

# Check the balance of our target values
print(y.value_counts())

#%%

# Create X_train, X_test, y_train, y_test
from sklearn.model_selection import train_test_split
# from library.module import class

X_train, X_test, y_train, y_test = train_test_split(X, 
                                                    y, 
                                                    # Controls the shuffling applied to the data before applying the split.
                                                    # Pass an int for reproducible output across multiple function calls.
                                                    random_state=1, 
                                                    # If not None, data is split in a stratified fashion, using this as the class labels
                                                    # https://www.scribbr.com/methodology/stratified-sampling/
                                                    stratify=y)

#%%
#%%

# Data Pre-Processing

# Scale the training and testing data using the StandardScaler from sklearn. 
# Remember that when scaling the data, you only scale the features data (X_train and X_testing).

#%%
#%% 1 of 3 in JL cell # 9

# Create the StandardScaler instance to normalize the values individually, before applying the ML model, to get it w/n distr of mean value of 0 and std dev of 1
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
#%% 2 of 3 in JL cell # 9

# Dummy Encoding (Binary Encoded Data)¶
# .get_dummies avoids this hassle entirely. All are 0s and 1s.  [from #1b exercise]
# Binary encoding using Pandas (single column)
# loans_binary_encoded = pd.get_dummies(loans_df, columns=["gender"])
# print(loans_binary_encoded.head())

new_X_train=pd.get_dummies(X_train, columns=["homeowner"])
new_X_test=pd.get_dummies(X_test, columns=["homeowner"])

#%% 3 of 3 in JL cell # 9

scaler.fit(new_X_train)
print(f'the scaler mean is {scaler.mean_}')

#%% # 10 JL a

# Fit the Standard Scaler with the training data
# When fitting scaling functions, only train on the training dataset

#%% # 10 JL b


# Creating the scaler instance
data_scaler = StandardScaler()

# Fitting the scaler
data_scaler.fit(new_X_train)

#%%

# Scale the training and testing data

# loans_data_scaled = data_scaler.transform(new_x_train)
# loans_data_scaled[:5]

X_train_scaled = data_scaler.transform(new_X_train)
X_test_scaled = data_scaler.transform(new_X_test)

#%%
#%%

# Simple Logistic Regression¶


#%%
#%%

from sklearn.linear_model import LogisticRegression
model = LogisticRegression(solver='lbfgs', random_state=1)
model.fit(X_train_scaled, y_train)

#%%

# Calculated the balanced accuracy score
from sklearn.metrics import balanced_accuracy_score
y_pred = model.predict(X_test_scaled)
print(f'The logistic-regression model balanced accuracy score is: {balanced_accuracy_score(y_test, y_pred)}')

#%%

# Display the confusion matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
print(cm)
#confusion_matrix(y_test, y_pred)

#%%

plt.clf()
plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Wistia)
classNames = ['Negative','Positive']
plt.title('Versicolor or Not Versicolor Confusion Matrix - Test Data')
plt.ylabel('True label')
plt.xlabel('Predicted label')
tick_marks = np.arange(len(classNames))
plt.xticks(tick_marks, classNames, rotation=45)
plt.yticks(tick_marks, classNames)
s = [['TN','FP'], ['FN', 'TP']]
for i in range(2):
    for j in range(2):
        plt.text(j,i, str(s[i][j])+" = "+str(cm[i][j]))
plt.show()

#%%

# Print the imbalanced classification report
from imblearn.metrics import classification_report_imbalanced
print(classification_report_imbalanced(y_test, y_pred))

#%%
#%%

# Oversampling

# In this section, you will compare two oversampling algorithms to determine which algorithm results in the best performance. You will oversample the data using the naive random oversampling algorithm and the SMOTE algorithm. For each algorithm, be sure to complete the folliowing steps:

#    View the count of the target classes using Counter from the collections library.
#    Use the resampled data to train a logistic regression model.
#    Calculate the balanced accuracy score from sklearn.metrics.
#    Print the confusion matrix from sklearn.metrics.
#    Generate a classication report using the imbalanced_classification_report from imbalanced-learn.

# Note: Use a random state of 1 for each sampling algorithm to ensure consistency between tests

#%%
#%%

# Naive Random Oversampling

#%%
#%%

# Resample the training data with the RandomOversampler
from imblearn.over_sampling import RandomOverSampler

ros = RandomOverSampler(random_state=1)
X_resampled, y_resampled = ros.fit_resample(X_train_scaled, y_train)

# View the count of target classes with Counter
print(Counter(y_resampled))

#%% JL # 17 A

# Train the Logistic Regression model using the resampled data
from sklearn.linear_model import LogisticRegression

#%% JL # 17 B


model = LogisticRegression(solver='lbfgs', random_state=1)
print(model.fit(X_resampled, y_resampled))

#%%

# Calculated the balanced accuracy score

from sklearn.metrics import balanced_accuracy_score
y_pred = model.predict(X_test_scaled)
print(f'The naive-random-oversampling balanced accuracy score is: {balanced_accuracy_score(y_test, y_pred):.4}')
#%%

# Display the confusion matrix
from sklearn.metrics import confusion_matrix

y_pred = model.predict(X_test_scaled)
cm = confusion_matrix(y_test, y_pred)
print(cm)


#%%

plt.clf()
plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Wistia)
classNames = ['Negative','Positive']
plt.title('naive-random-oversampling Confusion Matrix')
plt.ylabel('True label')
plt.xlabel('Predicted label')
tick_marks = np.arange(len(classNames))
plt.xticks(tick_marks, classNames, rotation=45)
plt.yticks(tick_marks, classNames)
s = [['TN','FP'], ['FN', 'TP']]
for i in range(2):
    for j in range(2):
        plt.text(j,i, str(s[i][j])+" = "+str(cm[i][j]))
plt.show()

#%%

# Print the imbalanced classification report
from imblearn.metrics import classification_report_imbalanced

print(classification_report_imbalanced(y_test, y_pred))

#%%
#%%

# SMOTE Oversampling

#%%
#%%

# Resample the training data with SMOTE
from imblearn.over_sampling import SMOTE
from collections import Counter
X_resampled, y_resampled = SMOTE(random_state=1, sampling_strategy=1.0).fit_resample(X_train_scaled, y_train)
# View the count of target classes with Counter
print(Counter(y_resampled))

#%% JL # 39 A

# Train the Logistic Regression model using the resampled data
from sklearn.linear_model import LogisticRegression

#%% JL # 39 B


model = LogisticRegression(solver='lbfgs', random_state=1)
print(model.fit(X_resampled, y_resampled))

#%%

# Calculated the balanced accuracy score
from sklearn.metrics import balanced_accuracy_score
y_pred = model.predict(X_test_scaled)
print(f'The smote-oversampling balanced accuracy score is: {balanced_accuracy_score(y_test, y_pred):.4}')

#%%

# Display the confusion matrix
y_pred = model.predict(X_test_scaled)
cm = confusion_matrix(y_test, y_pred)
print(cm)
#print(confusion_matrix(y_test, y_pred))

#%%

plt.clf()
plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Wistia)
classNames = ['Negative','Positive']
plt.title('The smote-oversampling  Confusion Matrix')
plt.ylabel('True label')
plt.xlabel('Predicted label')
tick_marks = np.arange(len(classNames))
plt.xticks(tick_marks, classNames, rotation=45)
plt.yticks(tick_marks, classNames)
s = [['TN','FP'], ['FN', 'TP']]
for i in range(2):
    for j in range(2):
        plt.text(j,i, str(s[i][j])+" = "+str(cm[i][j]))
plt.show()

#%%

# Print the imbalanced classification report
from imblearn.metrics import classification_report_imbalanced

print(classification_report_imbalanced(y_test, y_pred))

#%%
#%%

# Undersampling

#%%
#%%

# In this section, you will test an undersampling algorithm to determine which algorithm results in the best performance compared to the oversampling algorithms above. You will undersample the data using the Cluster Centroids algorithm and complete the folliowing steps:

#    View the count of the target classes using Counter from the collections library.
#    Use the resampled data to train a logistic regression model.
#    Calculate the balanced accuracy score from sklearn.metrics.
#    Display the confusion matrix from sklearn.metrics.
#    Generate a classication report using the imbalanced_classification_report from imbalanced-learn.

# Note: Use a random state of 1 for each sampling algorithm to ensure consistency between tests

#%%

# Resample the data using the ClusterCentroids resampler
from imblearn.under_sampling import ClusterCentroids

cc = ClusterCentroids(random_state=1)
X_resampled, y_resampled = cc.fit_resample(X_train_scaled, y_train)

# View the count of target classes with Counter
Counter(y_resampled)

#%%

# Train the Logistic Regression model using the resampled data
from sklearn.linear_model import LogisticRegression
model = LogisticRegression(solver='lbfgs', random_state=1)
model.fit(X_resampled, y_resampled)

#%%

# Calculate the balanced accuracy score
from sklearn.metrics import balanced_accuracy_score

y_pred = model.predict(X_test_scaled)
print(f'The undersampling balanced accuracy score is: {balanced_accuracy_score(y_test, y_pred):.4}')

#%%

# Display the confusion matrix
from sklearn.metrics import confusion_matrix
y_pred = model.predict(X_test_scaled)
cm = confusion_matrix(y_test, y_pred)
print(cm)
# print(confusion_matrix(y_test, y_pred))

#%%

plt.clf()
plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Wistia)
classNames = ['Negative','Positive']
plt.title('The undersampling Confusion Matrix')
plt.ylabel('True label')
plt.xlabel('Predicted label')
tick_marks = np.arange(len(classNames))
plt.xticks(tick_marks, classNames, rotation=45)
plt.yticks(tick_marks, classNames)
s = [['TN','FP'], ['FN', 'TP']]
for i in range(2):
    for j in range(2):
        plt.text(j,i, str(s[i][j])+" = "+str(cm[i][j]))
plt.show()

#%%

# Print the imbalanced classification report
from imblearn.metrics import classification_report_imbalanced
print(classification_report_imbalanced(y_test, y_pred))

#%%
#%%

Combination (Over and Under) Sampling

#%%
#%%

# In this section, you will test a combination over- and under-sampling algorithm to determine if the algorithm results in the best performance compared to the other sampling algorithms above. You will resample the data using the SMOTEENN algorithm and complete the folliowing steps:

#    View the count of the target classes using Counter from the collections library.
#    Use the resampled data to train a logistic regression model.
#    Calculate the balanced accuracy score from sklearn.metrics.
#    Display the confusion matrix from sklearn.metrics.
#    Generate a classication report using the imbalanced_classification_report from imbalanced-learn.

# Note: Use a random state of 1 for each sampling algorithm to ensure consistency between tests

#%%

# Resample the training data with SMOTEENN
from imblearn.combine import SMOTEENN

sm = SMOTEENN(random_state=1)
X_resampled, y_resampled = sm.fit_resample(X_train_scaled, y_train)

# View the count of target classes with Counter
print(Counter(y_resampled))

#%%

# Train the Logistic Regression model using the resampled data
model = LogisticRegression(solver='lbfgs', random_state=1)
model.fit(X_resampled, y_resampled)

#%%

# Calculate the balanced accuracy score
from sklearn.metrics import balanced_accuracy_score

y_pred = model.predict(X_test_scaled)
print(f'The combination over/under sampling balanced accuracy score is: {balanced_accuracy_score(y_test, y_pred):.4f}')

#%%

# Display the confusion matrix
y_pred = model.predict(X_test_scaled)
cm = confusion_matrix(y_test, y_pred)
print(cm)
#print(confusion_matrix(y_test, y_pred))

#%%

plt.clf()
plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Wistia)
classNames = ['Negative','Positive']
plt.title('The combination over/under sampling Confusion Matrix')
plt.ylabel('True label')
plt.xlabel('Predicted label')
tick_marks = np.arange(len(classNames))
plt.xticks(tick_marks, classNames, rotation=45)
plt.yticks(tick_marks, classNames)
s = [['TN','FP'], ['FN', 'TP']]
for i in range(2):
    for j in range(2):
        plt.text(j,i, str(s[i][j])+" = "+str(cm[i][j]))
plt.show()

#%%

# Print the imbalanced classification report
from imblearn.metrics import classification_report_imbalanced

print(classification_report_imbalanced(y_test, y_pred))

#%%

# Final Questions

1. Which model had the best balanced accuracy score?

   All models produced the same balanced accuracy score of .9947, which is very good since it's on a scale of 0 to 1.

2. Which model had the best recall score?

    All models produced the same recall score ('rec' column of imbalanced classification report) of .99.

3. Which model had the best geometric mean score?

    All models produced the same recall score ('geo' column of imbalanced classification report) of .99.