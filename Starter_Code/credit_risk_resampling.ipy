# -*- coding: utf-8 -*-
"""
Created on Sat Nov 27 12:40:53 2021

@author: CS_Knit_tinK_SC
"""

# Credit Risk Resampling Techniques

#%%

import warnings
warnings.filterwarnings('ignore')

#%%

import numpy as np
import pandas as pd
from pathlib import Path
from collections import Counter

#%%
#%%

# Read the CSV into DataFrame

#%%
#%%

# Load the data
file_path="C:/Users/CS_Knit_tinK_SC/Documents/GitHub/HW_8_ML_Conf_Imb_Inputs_U11/Resources/lending_data.csv"
#file_path = Path('Resources/lending_data.csv')
df = pd.read_csv(file_path)
print(df.head())

#%%
#%%

# Split the Data into Training and Testing¶

#%%
#%%

# Create our features
X = df.drop(columns="loan_status")

# Create our target
y = df["loan_status"]

#%%

print(X.describe())

#%%

print(y.head())

#%%

# Check the balance of our target values
print(y.value_counts())

#%%

# Create X_train, X_test, y_train, y_test
from sklearn.model_selection import train_test_split
# from library.module import class

X_train, X_test, y_train, y_test = train_test_split(X, 
                                                    y, 
                                                    # Controls the shuffling applied to the data before applying the split.
                                                    # Pass an int for reproducible output across multiple function calls.
                                                    random_state=1, 
                                                    # If not None, data is split in a stratified fashion, using this as the class labels
                                                    # https://www.scribbr.com/methodology/stratified-sampling/
                                                    stratify=y)

#%%
#%%

# Data Pre-Processing

# Scale the training and testing data using the StandardScaler from sklearn. 
# Remember that when scaling the data, you only scale the features data (X_train and X_testing).

#%%
#%%

# Create the StandardScaler instance to normalize the values individually, before applying the ML model, to get it w/n distr of mean value of 0 and std dev of 1
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
#%%

# Dummy Encoding (Binary Encoded Data)¶
# .get_dummies avoids this hassle entirely. All are 0s and 1s.  [from #1b exercise]
# Binary encoding using Pandas (single column)
# loans_binary_encoded = pd.get_dummies(loans_df, columns=["gender"])
# print(loans_binary_encoded.head())

new_X_train=pd.get_dummies(X_train, columns=["homeowner"])
new_X_test=pd.get_dummies(X_test, columns=["homeowner"])

#%%

scaler.fit(new_X_train)
print(f'the scaler mean is {scaler.mean_}')

#%%

# Fit the Standard Scaler with the training data
# When fitting scaling functions, only train on the training dataset

#%%


# Creating the scaler instance
data_scaler = StandardScaler()

# Fitting the scaler
data_scaler.fit(new_X_train)

#%%

# Scale the training and testing data

# loans_data_scaled = data_scaler.transform(new_x_train)
# loans_data_scaled[:5]

X_train_scaled = data_scaler.transform(new_X_train)
X_test_scaled = data_scaler.transform(new_X_test)

#%%
#%%

# Simple Logistic Regression¶


#%%
#%%

from sklearn.linear_model import LogisticRegression
model = LogisticRegression(solver='lbfgs', random_state=1)
model.fit(X_train_scaled, y_train)

#%%

# Calculated the balanced accuracy score
from sklearn.metrics import balanced_accuracy_score
y_pred = model.predict(X_test_scaled)
balanced_accuracy_score(y_test, y_pred)

#%%

# Display the confusion matrix
from sklearn.metrics import confusion_matrix
confusion_matrix(y_test, y_pred)

#%%

# Print the imbalanced classification report
from imblearn.metrics import classification_report_imbalanced
print(classification_report_imbalanced(y_test, y_pred))

#%%
#%%

# Oversampling

# In this section, you will compare two oversampling algorithms to determine which algorithm results in the best performance. You will oversample the data using the naive random oversampling algorithm and the SMOTE algorithm. For each algorithm, be sure to complete the folliowing steps:

#    View the count of the target classes using Counter from the collections library.
#    Use the resampled data to train a logistic regression model.
#    Calculate the balanced accuracy score from sklearn.metrics.
#    Print the confusion matrix from sklearn.metrics.
#    Generate a classication report using the imbalanced_classification_report from imbalanced-learn.

# Note: Use a random state of 1 for each sampling algorithm to ensure consistency between tests

#%%
#%%

# Naive Random Oversampling

#%%
#%%

# Resample the training data with the RandomOversampler
from imblearn.over_sampling import RandomOverSampler

ros = RandomOverSampler(random_state=1)
X_resampled, y_resampled = ros.fit_resample(X_train_scaled, y_train)

# View the count of target classes with Counter
print(Counter(y_resampled))

#%%

# Train the Logistic Regression model using the resampled data
from sklearn.linear_model import LogisticRegression

#%%

# Calculated the balanced accuracy score
model = LogisticRegression(solver='lbfgs', random_state=1)
print(model.fit(X_resampled, y_resampled))

#%%

# Display the confusion matrix
from sklearn.metrics import confusion_matrix

y_pred = model.predict(X_test_scaled)

#%%

# Print the imbalanced classification report
print(confusion_matrix(y_test, y_pred))

#%%



