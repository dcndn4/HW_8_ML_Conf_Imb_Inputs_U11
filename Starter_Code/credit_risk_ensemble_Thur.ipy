# -*- coding: utf-8 -*-
"""
Created on Sun Nov 28 14:56:34 2021

@author: CS_Knit_tinK_SC
"""

# Ensemble Learning

#%%
# Initial Imports

#%%

import warnings
warnings.filterwarnings('ignore')

#%%

import numpy as np
import pandas as pd
from pathlib import Path
from collections import Counter
import seaborn as sns
%matplotlib inline

#%%

from sklearn.metrics import balanced_accuracy_score
from sklearn.metrics import confusion_matrix
from imblearn.metrics import classification_report_imbalanced

#%%

# Read the CSV and Perform Basic Data Cleaning

#%%

# Load the data
#file_path = Path('Resources/LoanStats_2019Q1.csv')
file_path="C:/Users/CS_Knit_tinK_SC/Documents/GitHub/HW_8_ML_Conf_Imb_Inputs_U11/Resources/LoanStats_2019Q1.csv"
df = pd.read_csv(file_path)

# Preview the data
print(df.head())

#%%
#%%

# Split the Data into Training and Testing

#%%
#%%

# Create our features
X = df.drop(columns=["loan_status", "issue_d", "pymnt_plan", "initial_list_status", "next_pymnt_d"])

# Create our target
y = df["loan_status"]

#%%

print(X.describe())

#%%

# Check the balance of our target values
print(y.head())
print(y.value_counts())

#%%

# Create X_train, X_test, y_train, y_test
from sklearn.model_selection import train_test_split
# from library.module import class

X_train, X_test, y_train, y_test = train_test_split(X, 
                                                    y, 
                                                    # Controls the shuffling applied to the data before applying the split.
                                                    # Pass an int for reproducible output across multiple function calls.
                                                    random_state=1, 
                                                    # If not None, data is split in a stratified fashion, using this as the class labels
                                                    # https://www.scribbr.com/methodology/stratified-sampling/
                                                    stratify=y)

#%%
#%%

# Data Pre-ProcessingÂ¶

# Scale the training and testing data using the StandardScaler from sklearn. Remember that when scaling the data, you only scale the features data (X_train and X_testing).


#%%
#%%

# Create the StandardScaler instance to normalize the values individually, before applying the ML model, to get it w/n distr of mean value of 0 and std dev of 1
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
new_X_train=pd.get_dummies(X_train, columns=["home_ownership", "verification_status", "hardship_flag", "debt_settlement_flag", "application_type"])
new_X_test=pd.get_dummies(X_test, columns=["home_ownership", "verification_status", "hardship_flag", "debt_settlement_flag", "application_type"])
scaler.fit(new_X_train)
print(f'the scaler mean is {scaler.mean_}')

#%%

# Fit the Standard Scaler with the training data
# When fitting scaling functions, only train on the training dataset
data_scaler = StandardScaler()

print(data_scaler.fit_transform(new_X_train))

#%%

# Scale the training and testing data
X_train_scaled = data_scaler.transform(new_X_train)
X_test_scaled = data_scaler.transform(new_X_test)

#%%
#%%
#%%

# Ensemble Learners

# In this section, you will compare two ensemble algorithms to determine which algorithm results in the best performance. You will train a Balanced Random Forest Classifier and an Easy Ensemble classifier . For each algorithm, be sure to complete the folliowing steps:
    
#%%
#%%
#%%

# Resample the training data with the BalancedRandomForestClassifier
# from sklearn.ensemble import RandomForestClassifier

### more to copy over yet

#%%
#%%

# Balanced Random Forest Classifier

#%%
#%%
from numpy import mean
from imblearn.ensemble import BalancedRandomForestClassifier
brf = BalancedRandomForestClassifier(n_estimators=1000, random_state=1)
print(brf.fit(X_train_scaled, y_train))

#%%


print(brf.feature_importances_)

#%%

# Print the imbalanced classification report
y_pred_rf = brf.predict(X_test_scaled)
print(classification_report_imbalanced(y_test, y_pred_rf))

#%%


print(brf.predict(X_train_scaled))

#%%

# Calculated the balanced accuracy score
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import balanced_accuracy_score


y_pred = model.predict(X_test_scaled)
print(f'the random forest classifier balanced accuracy score is: {balanced_accuracy_score(y_test, y_pred):.4f}')
