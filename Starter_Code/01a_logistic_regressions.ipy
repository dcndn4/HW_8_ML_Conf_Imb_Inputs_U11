# -*- coding: utf-8 -*-
"""
Created on Tue Nov 16 18:11:04 2021

@author: CS_Knit_tinK_SC
"""

# 6:50 PM
# 7:00 PM
import os; print(os.path.dirname(os.getcwd()).split('\\')[-1])

#%%

# 01-Ins_Logistic_Regression

# Correction

# Last class, I said using hyperparameters distinguished between the "dials" in ML, and math equation parameters. I confused math equation parameters for terms.

# The more obvious thing is that hyperparameters distinguishes them from function parameters.

# def something(param1, param2, param3):

# Comment

# Logistic Regression is a statistical method for predicting binary outcomes.

# e.g. "yes"/"no" or "high credit risk"/"low credit risk".

# These are categories that translate to probability of being a 0 or a 1

# We can calculate logistic regression by adding an activation function as the final step to our linear model.

# This converts the linear regression output to a probability.

#%%

import matplotlib.pyplot as plt
import pandas as pd

#%%

# Generate some data

# Comment

# centers ~= "centroids"

# Python culture quiz: Why 42?

# Random state, random seed...think about it like this. We are working with randomly generated data, but we still want reproducability. Controlling the random seed let's us "shuffle a deck of cards", but in the exact same order. If we change from 42 to 43, we get an entirely new shuffle, but it remains reproducible.

# This helps us overcome the Replication crisis.

#    A 2016 poll of 1,500 scientists conducted by Nature reported that 70% of them had failed to reproduce at least one other scientist's experiment (including 87% of chemists, 77% of biologists, 69% of physicists and engineers, 67% of medical researchers, 64% of earth and environmental scientists, and 62% of all others), while 50% had failed to reproduce one of their own experiments, and less than 20% had ever been contacted by another researcher unable to reproduce their work. Only a minority had ever attempted to publish a replication, and while 24% had been able to publish a successful replication, only 13% had published a failed replication, and several respondents that had published failed replications noted that editors and reviewers demanded that they play down comparisons with the original studies.[6][9] In 2009, 2% of scientists admitted to falsifying studies at least once and 14% admitted to personally knowing someone who did. Such misconduct was, according to one study, reported more frequently by medical researchers than by others.[10] A 2021 study found that papers in leading journals with findings that can't be replicated tend to be cited more than reproducible science. Results that are published unreproducibly – or not in a replicable sufficiently transparent way – are more likely to be wrong and may slow progress. The authors also put forward possible explanations for this state of affairs.[11][12]

# https://www.econtalk.org/brian-nosek-on-the-reproducibility-project/

#    Reproducibility Project-an effort to reproduce the findings of 100 articles in three top psychology journals. Nosek talks about the findings and the implications for academic publishing and the reliability of published results.

# Anyways, the below randomly generates clustered data.

#%%

from sklearn.datasets import make_blobs

X, y = make_blobs(centers=2, random_state=42)

print(f"Labels: {y[:10]}")
print(f"Data: {X[:10]}")

#%%

# Comment

# Centering is a pre-processing step in advanced analytics. 
# It improves the performance of logistic regression by ensuring that 
# all observations cluster around the same starting mean value.

#%%

# Visualizing both classes
plt.scatter(X[:, 0], X[:, 1], c=y)  # column 0 and column 1

#%%
#%%

# Split our data into training and testing

#%%
#%%

from sklearn.model_selection import train_test_split
# from library.module import class

X_train, X_test, y_train, y_test = train_test_split(X, 
                                                    y, 
                                                    # Controls the shuffling applied to the data before applying the split.
                                                    # Pass an int for reproducible output across multiple function calls.
                                                    random_state=1, 
                                                    # If not None, data is split in a stratified fashion, using this as the class labels
                                                    # https://www.scribbr.com/methodology/stratified-sampling/
                                                    stratify=y)

#%%

# Create a Logistic Regression Model

# Comment

# Solver optimized learning and computation. There are many different options. See this SO discussion for more:

# https://stackoverflow.com/questions/38640109/logistic-regression-python-solvers-definitions


#%%

from sklearn.linear_model import LogisticRegression
classifier = LogisticRegression(solver='lbfgs', random_state=1)
print(classifier)

#%%

# Fit (train) or model using the training data¶

# Comment

# Giving data to the model to learn from = training/fitting.


#%%

# Train the data
classifier.fit(X_train, y_train)

#%%

# Validate the model using the test data

# Comment

# Scoring = comparing the guesses to the actuals.

# The model has never seen the test data, so this is where you see how well it might do in the real world. It's considered "new" data.

#%%

# Score the model
print(f"Training Data Score: {classifier.score(X_train, y_train)}")
print(f"Testing Data Score: {classifier.score(X_test, y_test)}")

#%%

# Comment

#  If training accuracy is significantly higher than test, then perhaps the model is overfit.

# The goal is to have both accuracy scores as close to each other as possible.

# Once the model is accurate for both train and test, give it new data and predict away!

#%%
#%%

# Make predictions

#%%
#%%

# Predict outcomes for test data set
predictions = classifier.predict(X_test)
pd.DataFrame({"Prediction": predictions, "Actual": y_test})

#%%

# Generate a new data point (the red circle)
import numpy as np
new_data = np.array([[-2, 6]])
plt.scatter(X[:, 0], X[:, 1], c=y)
plt.scatter(new_data[0, 0], new_data[0, 1], c="r", marker="o", s=100)
# new_data  # this is a new observation

#%%

# Predict the class (purple or yellow) of the new data point
predictions = classifier.predict(new_data)
print("Classes are either 0 (purple) or 1 (yellow)")
print(f"The new point was classified as: {predictions}")

#%%